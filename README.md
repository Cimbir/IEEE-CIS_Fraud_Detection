# IEEE-CIS_Fraud_Detection

ამ პროექტის მიზანია ტრანზაქციების შემოწმება, რათა მაქსიმალურად კარგად გაირკვას თუ რომელი არის თაღლითობა და რომელი არა. დატა შემოდის ტრანზაქციებისა და მომხმარებლების სია, ხოლო პასუხი უნდა დაბრუნდეს ის ალბათობა, რა ალბათობითაც შეიძლება რაიმე ტრანზაქცია იყოს თაღლითობა. evaluation ხდება ROC Area Under Curve-ს მიხედვით. ამ ამოცანის გადასაჭრელად უმეტესად ვიყენებ ხის სტრუქტურის მქონე სხვადასხვა მოდელებს. თავიდან უბრალოდ ვცდილობ მოდელის მოქმედებას გავუგო სხვადასხვა cleaning, engineering და selecting მეთოდებით. მაგის შემდეგ პროცესს შემდეგნაირად მივუყვები: cleaning -> feature engineering -> feature selection (correlation filter) -> initial model training -> feature selection (feat. imp. or shap) -> random search -> final model training -> deep evaluation -> mlflow logging (run-ებს, რომლებსაც აქვთ '_Search_Best' სუფიქსი, არის უკვე საბოლოო მოდელები, ხოლო სხვა run-ები უბრალოდ სატესტოები ან under/overfit-ის საჩვენებლადაა)

## რეპოზიტორიის სტრუქტურა

* `model_experiment_logistic_regression.py` - Logistic Regression მოდელი

* `model_experiment_decision_tree.py` - Decision Tree მოდელი

* `model_experiment_random_forest.py` - Random Forest მოდელი

* `model_experiment_gradient_boost.py` - Gradient Boost მოდელი

* `model_experiment_adaboost.py` - AdaBoost მოდელი

* `model_experiment_xgboost.py` - XGBoost მოდელი

* `model_inference.py` - submission-ის დაგენერირება

## Feature Engineering

ყველა მოდელის გასუფთავების პროცესი ერთმანეთს გავს: თავიდან ისეთ სვეტებს ვშლი, რომლებიც ნახევარზე მეტ NaN-ს შეიცავს(ასეთი ბევრი იყო და მაგ სვეტების ინფორმაციის შენარჩუნებას ჯობია გადავაგდოთ რათა ხელი არ შეგვიშალოს), შემდეგ სტრიქონებს რომლებიც ნახევარზე მეტს NaN-ს შეიცავს(ასეთი სტრიქონები მაინცდამაინც არ იქნება კარგი დასასწავლად, რადგან ისედაც მწირი ინფორმაცია ექნება), შემდეგ NaN-ებს ვამუშავებთ გადმოცემული მოდელებით: რიცხვითს ცალკე და კატეგორიულს ცალკე(დამოკიდებულია მოდელზე). 
თავდაპირველად ბოლოს კიდევ ვფილტრავდი outlier-ებს 2 მეთოდით: IQR და Z-score. ამ მიდგომამ კარგად ვერ გაამართლა. თავდაპირველად აღმოჩნდა, რომ თუ გამოვიყენებდი iqr-ს 0.25 ზღვარით, მაშინ ყველა isFraud=1 დატა ქრებოდა, რაც იმას ნიშნავს, რომ ამ კონკრეტული ფილტრის მიხედვით ყველა თაღლითობა outlier-ია. zscore-მა უარესი შედეგი დადო, რომ საერთოდ ყველა დატა წაშალა 3 ზღვარზე. გამოსწორებას ვეცადე იმითი, რომ მხოლოდ წამეშალა outlier-ები isFraud=0-დან, მაგრამ ამან გამოიწვია კიდევ 2 პრობლენა: სწორი ტრანზაქციების განაწილება ირღვეოდა და მოდელების ტესტირებისას უარესი შედეგები ჰქონდა ასეთ დატას, რაც იმისი ბრალია, რომ სწორ ტრანზაქციებში outlier-ებს მაინც აქვთ ის ინფორმაცია, რომელიც მოდელს დაეხმარება, როგორც თაღლითურ ტრანზქციებსაც აქვთ
დატა რაც გადმოგვეცემა, მაქედან მხოლოდ ტრანზაქციებს ვიყენებ, რადგან მომხმარებლების სია რომ დამემატებინა, უმეტესობის გადაგდება მომიწევდა ისედაც, ამიტომ არ ექნებოდა აზრი

### დამუშავება

| Model                        | Num Imputer | Cat Imputer      | Encoder |
|------------------------------|-------------|------------------|---------|
| Logistic Regression          | Mean        | Most Frequent    | WOE     |
| Decision Tree Classifier     | Mean        | Most Frequent    | One Hot |
| Random Forest Classifier     | Mean        | Const('Missing') | One Hot |
| Gradient Boosting Classifier | Const(0)    | Const('Missing)  | Target  |
| AdaBoost Classifier          | Const(0)    | Const('Missing') | Target  |
| XGBoost Classifier           | X           | X                | WOE     |

## Feature Selection

feature selection-ს დროს ყოველთვის ასე ვაკეთებდი: correlation filter-ით ვფილტრავ, ყველა feature-ით ვასწავლი მოდელს, სხვადასხვა მეთოდით ვპოულობდი საუკეთესო სვეტებს, თავიდან ვასწავლი მოდელს. Logistic Regression-სთვის გამოვიყენე RFE, თუმცა ხის სტრუქტურებისთვის ვიყენებდი მხოლოდ shap-ს თუ მოდელი სწრაფად ტრენინგდებოდა და feature importance-ს სხვა შემთხვევაში. ამისი მიზეზი ისაა, რომ RFE-ს დიდი დრო სჭირდებოდა სამუშაოდ. ამასთან ერთად, shap და feature importance უკეთეს წარმოდგენას იძლეოდა თვითონ მოდელის სტრუქტურაზე, რის გამოც ბოლოს ეს 2 მეთოდი შევარჩიე. მართალია shap უკეთ ჰენდლავს კორელაციას, თუმცა მაინც ვფილტრავ ორივე მეთოდის გაშვებამდე

## Training

### მოდელები
ჯამში სულ 6 მოდელი განვიხილე: `Logistic Regression`, `Decision Tree`, `Random Forest`, `Gradient Boost`, `AdaBoost`, `XGBoost`

### ჰიპერპარამეტრების ოპტიმიზაცია
დროის მოგების მიზნით grid search-ის მაგივრად, ვაკეთებდი random search-ს და საუკეთესო მაჩვენებლების მქონე მოდელების პარამეტრებზე დაკვირვებით ვაგებდი ცალკე საუკეთესო მოდელს. Cross Validation-სთვის ვიყენებ StratifiedKFold-ს, რათა შევინარჩუნო დატას განაწილება ყველა fold-ში. 3 fold მაქვს აღებული, რადგან უფრო მეტით დიდ დროს ანდომებს. ინტერნეტში მოძიებული მასალის მიხედვით დაუბალანსებელ დატაში გამოიყენებენ ხოლმე SMOTE-ს უმცირესობაში არსებული დატის ხელოვნურად გასაზრდელად. ეს მიდგომა შესაძლოა აქაც გამომდგარიყო, მაგრამ პრობლემა ისაა, რომ არსებული დატა სინამდვილეშიც იშვიათია. ტრანზაქციების უმეტესობა ცხადია ვალიდურია. ჩვენ გვაინტერესებს ის მცირე ყალბის გარჩევამ რომელიც ამ ბევრ ვალიდურში იქნება მოთავსებული, რის გამოც ბოლოს აღარ ვიყენებ ხოლმე არსად, იმის მიუხედავად, რომ ზოგ წინა მოდელში უკეთესი შედეგიც კი დაუდია (მაგრამ მანდ სხვა ფაქტორების გამო) და ახლა მხოლოდ კოდში არის შემორჩენილი. RandomSearchCV-ს დროს დავაფიქსირე, რომ როდესაც გასატესტი რეინჯი რაც უფრო მაღალი იყო, მით უფრო მაღალ მნიშვნელობებს ანიჭებდა უპირატესობას, რის გამოც საუკეთესო მოდელში აღინიშნებოდა overfitting(რაც შესაძლებელია mlflow-ზე დალოგილ search არტეფაქტებში ნახვა). ამის გამო ჰიპერპარამეტრების შესაძლო მნიშვნელობების რეინჯს ვწევდი დაბლა, რათა აერჩია უფრო მარტივი მოდელები და ყველა კარგი მოდელი არ ყოფილიყო overfitting-ის გამო

---

### Logistic Regression

ეს საერთოდ არ ეხება ხის სტრუქტურას, თუმცა მაინც გავაკეთე მაქსიმალურად ბევრი მოდელის გასატესტად. ხის სტრუქტურებისგან განსხვავებით, ამას დასჭირდა StandardScaler-ის გამოყენება, და ამავროულად რეგულარიზაციის გასაზრდელად, C ჰიპერპარამეტრის შემცირებაც

#### Random Search Params (Logistic Regression)

| Parameter         | Range                                    |
|-------------------|------------------------------------------|
| scaler_variants   | [None, StandardScaler(), MinMaxScaler()] |
| model__C          | 0.1-10                                   |

#### Submission Result 
* private: 0.817354
* public: 0.850836

---

### Decision Tree

* 0 - საბაზისო მოდელი, სადაც ჯერ დიდად არაფერს ვითვალისწინებ. ძირითადად რომ ემუშავა კოდს მაგისთვის გავაკეთე
* 1 - სიღრმე 1-ზე დავაყენე underfit-ობის გამოსაკვეთად
* 2 - სიღრმეს ლიმიტი მოვაცილე, რამაც გამოიწვია overfit-ობა (სატრენინგო დატას სრულყოფილად ერგება)
* Best - ხელით გატესტვების შედეგად საუკეთესო მოდელი
* Search_Best - RandomSearchCV-ს შედეგად მიღწეული საუკეთესო მოდელი
* Underfit - იგივე რაც Search_Best, მაგრამ სიღრმე 1-ზეა დაყენებული underfit-ობის გამოსაკვეთად. ასე დატა მხოლოდ ერთხელ გაიყოფა საუკეთესო მახასიათებლებით და ეგრე განაწილდება, რაც ცხადია ამხელა დატაზე არ იქნება საკმარისად კომპლექსური
* Overfit - იგივე რაც Search_Best, მაგრამ სიღრმე არ შევზღუდე და pruning-ც გავუთიშე(ccp alpha = 0) overfit-ობის გამოსაკვეთად. ასე აიგება უზარმაზარი ხე, რომელიც სრულყოფილად შეეწყობა სატრენინგო დატას, მაგრამ სატესტოსტოზე დიდი ვარდნა ექნება, რადგან ისეთი რაღაცები ექნება კარგად დასწავლილი, რაც მახასიათებელია სატრენინგო დატასთვის და არ არის სინამდვილეში

ამ მაგალითებიდან კარგად ჩანს, რომ მარტივია ხის სტრუქტურის მოდელის under/overfit-ება. საკმარისია სიღრმე შეუმცირო და ვეღარ დაისწავლის ცვლადების კომპლექსურ კომბინაციებს ან სიღრმეს გაუზრდი და სატრენინგო დატის მახასიათებლებს დაისწავლის ზედმეტად მჭიდროდ

#### Random Search Params (Decision Tree)

| Parameter               | Range      |
|-------------------------|------------|
| model__max_depth        | 5-15       |
| model__min_samples_split| 5-50       |
| model__min_samples_leaf | 2-20       |
| model__max_leaf_nodes   | 50-200     |
| model__ccp_alpha        | 0-0.001    |

#### Submission Result 
* private: 0.820955
* public: 0.848389

---

### Random Forest

* 0 - საბაზისო
* 1 - bootstrap-ის გარეშე
* 2 - სიღრმე 15 -> 10
* 3 - class_weight {0:1, 1:10}
* 4 - class_weight {0:1, 1:5}
* 5 - class_weight {0:1, 1:20}
* 6 - class_weight {0:1, 1:5}, bootstrap-ის გარეშე
* Best - ხელით გატესტვების შედეგად საუკეთესო მოდელი
* Search_Best - RandomSearchCV-ს შედეგად მიღწეული საუკეთესო მოდელი

აქ მთავარი საპოვნელი იყო საუკეთესო class_weight, რომელიც ყველაზე უკეთ დაახასიათებდა დატას. როგორც აღმოჩნდა 1:5 ყოფილა ის ფარდობა, რომლითაც ყურადღება მიექცეოდა თაღლითობებს, მაგრამ ამავდროულად არ დაიკარგებოდა ის ფაქტიც, რომ იშვიათია. კიდევ ერთი დაკვირვება ის იყო, რომ კარგი მოდელების უმეტესობა იყო bootstrap-თ, რაც შეიძლება იმისი მიზეზი იყოს, რომ ამან გამოიწვია უფრო მრავალფეროვანი ხეების წარმოქმნა, რამაც შეამცირა overfit და შესაბამისად უფრო კომპლექსური ხეების შექმნა შეიძლებოდა დატის უკეთ დასასწავლად

#### Random Search Params (Random Forest)

| Parameter            | Range          |
|----------------------|----------------|
| model__n_estimators  | 50-100         |
| model__max_depth     | 5-10           |
| model__max_leaf_nodes| 50-200         |
| model__bootstrap     | [False, True]  |

#### Submission Result 
* private: 0.848202
* public: 0.877831

---

### Gradient Boosting

* 0 - საბაზისო, საკმაოდ ნელი
* 1 - დაჩქარება და feature-ების რაოდენობის შემცირება (10)
* 2 - feature-ების რაოდენობის გაზრდა (20)
* 3 - subsample-ის გაზრდა (0.8)
* 4 - feature-ების რაოდენობის გაზრდა (30)
* Search_Best - RandomSearchCV-ს შედეგად მიღწეული საუკეთესო მოდელი

აქ უფრო მეტად დავიწყე შერჩევა საუკეთესო feature-ების რაოდენობის. ამ და მომავალი მოდელებიდან გამომდინარე, საუკეთესო აღმოჩნდა 20-სა და 40-ს შორის, რის გამოც მოდელებში ვირჩევ 30 ყველაზე ძლიერ feature-ებს

#### Random Search Params (Gradient Boosting)

| Parameter            | Range     |
|----------------------|-----------|
| model__n_estimators  | 50-100    |
| model__learning_rate | 0.1-0.5   |
| model__max_depth     | 3-5       |
| model__max_leaf_nodes| 5-10      |
| model__subsample     | 0.6-0.8   |

#### Submission Result 
* private: 0.840198
* public: 0.874461

---

### AdaBoost

* 0 - საბაზისო
* 1 - feature-ების რაოდენობის გაზრდა (20 -> 40)
* 2 - n_estimators-ის გაზრდა (100 -> 200)
* 3 - learning_rate-ის შემცირება (0.5 -> 0.1)
* Search_Best - RandomSearchCV-ს შედეგად მიღწეული საუკეთესო მოდელი
* Underfit - იგივე რაც Search_Best, მაგრამ learning_rate 0.01-ზეა დაყენებული, ხოლო n_estimator = 10 underfit-ობის გამოსაკვეთად

ეს საკმაოდ ნელი მოდელი იყო. ბევრი ჰიპერპარამეტრის შეცვლა მომიწია რათა ნორმალურ დროში მომხდარიყო მოდელის ტრენინგი და საუკეთესოს გამორკვევა. ბოლოს გამოჩნდა, რომ არ არის საჭირო უამრავი ხის აგება იმისთვის რომ მოდელმა კარგად იმუშაოს

#### Random Search Params (AdaBoost)

| Parameter                      | Range            |
|--------------------------------|------------------|
| model__n_estimators            | 50-100           |
| model__learning_rate           | 0.1-0.5          |
| model__estimator__max_depth    | 1-3              |
| model__estimator__max_features | ['sqrt', 'log2'] |

#### Submission Result 
* private: 0.843113
* public: 0.867423

---

### XGBoost

* 0 - საბაზისო
* 1 - feature-ების რაოდენობის შემცირება (60 -> 20)
* 2 - feature-ების რაოდენობის გაზრდა (20 -> 40)
* 3 - smote-ის გარეშე, დიდად არაფერი შეცვლილა
* 4 - n_estimators-ის გაზრდა (300 -> 500)
* 5 - learning_rate-ის გაზრდა (0.05 -> 0.2) + n_estimators 400-ზე
* 6 - max_depth-ის გაზრდა (6 -> 10)
* 7 - max_depth 20, overfit-ობა ჩანს უკვე
* 8 - n_estimators 100, max_depth 1 - underfit-ობა
* 9 - gamma-ს გაზრდა (1.0 -> 2.0)
* 10 - RandomSearchCV-ს შედეგად მიღწეული საუკეთესო მოდელების საშუალო მოდელი
* 11 - IQR-ის გაწმენდის გარეშე დატას დამუშავება
* Best - ხელით გატესტვების შედეგად საუკეთესო მოდელი
* Search_Best - RandomSearchCV-ს შედეგად მიღწეული საუკეთესო მოდელი
* Underfit - იგივე რაც Search_Best, მაგრამ სიღრმე = 1, learning rate = 0.01 და n_estimators = 10 underfit-ობის გამოსაკვეთად
* Overfit - იგივე რაც Search_Best, მაგრამ სიღრმე = 4, learning rate = 1.0, gamma = 1(ნაკლები რეგულარიზაცია) და n_estimators = 300 overfit-ობის გამოსაკვეთად

ამ მოდელზე ბევრი ტესტირების შემდეგ გავიაზრე თუ დატა როგორ იცვლებოდა outlier-ების მიმართ გაფილტრვისას(რის შესახებაც ზევით მიწერია). ამის გარდაც, ისევე როგორც decision tree-ებში, აქაც შესაძლებელია under/overfitting, თუმცა აქ ამისი მეტი გზაა, მაგალითად ახლა learning rate, n_estimators, gamma. ამასთან ერთად, XGBoost-ის დამახასიათებელი თვისებაა, რომ ის ჰენდლავს NaN მნიშვნელობებს, რის გამოც აღარ ვიყენებ არანაირ მეთოდს რათა NaN-ები შევავსო

#### Random Search Params (XGBoost)

| Parameter            | Range     |
|----------------------|-----------|
| model__max_depth     | 2-5       |
| model__n_estimators  | 100-300   |
| model__learning_rate | 0.05-0.12 |
| model__gamma         | 1.0-3.0   |

#### Submission Result 
* private: 0.873814
* public: 0.905480


---

### საბოლოო მოდელის შერჩევა

საბოლოოდ შევარჩიე XGBoost, რადგან ტრენინგის დროს ყველაზე მაღალ შედეგებს დებდა და ამის გარდაც საკმაოდ მალე ახერხებდა დასწავლასაც სხვა მოდელებთან შედარებით. submission-ს დროსაც უფრო ნათელი გახდა, როდესაც ყველა სხვა მოდელთან შედარებით გაცილებით უკეთესი შეფასება აიღო


## MLflow Tracking

[ექსპერიმენტის ბმული](https://dagshub.com/Cimbir/IEEE-CIS_Fraud_Detection.mlflow/)

### პარამეტრების აღწერა

#### Cleaning

* `cols_to_remove` - სვეტების ჩამონათვალი, რომელიც უნდა წაიშალოს

* `row_nan_frac` - სტრიქონებში NaN-ების თანაფარდობა, რომელზეც ან მეტზეც უნდა წაიშალოს

* `outlier_handling` - outlier-ების გაწმენდის მეთოდი

* `outlier_thresh` - outlier-ების გაწმენდის ბარიერის მნიშვნელობა

#### Engineering

* `encoding` - ყველა კატეგორიული სვეტი რა მეთოდით უნდა იქნეს კოდირებული

#### Selection

* `corr_threshold` - კორელაციის ბარიერი, რომლის მიხედვითაც უნდა მოხდეს სვეტების წაშლა

* `n_features_to_select` - feature-ების რაოდენობა, რომელიც უნდა დარჩეს

* `to_stay` - სვეტების ჩამონათვალი, რომელიც რჩება

* `to_remove` - სვეტების ჩამონათვალი, რომელიც უნდა წაიშალოს

* `feature_selection_method` - სვეტების შერჩევის მეთოდი (shap ან feature importance)

#### Training

* `sampler` - სამპლირების მეთოდი

* `model` - მოდელის ტიპი

* `best_X` - ამ კონკრეტული მოდელის პარამეტრის მნიშვნელობა

* `model__X` - RandomSearchCV-ს დროს არჩეული მნიშვნელობების რეინჯი

### მეტრიკების აღწერა

* `test_roc_auc`:`train_roc_auc`- ROC AUC-ის შეფასება

* `test_f1_macro`:`train_f1_macro` - F1-ის შეფასება

* `test_recall_macro`:`train_recall_macro` - Recall-ის შეფასება

* `test_precision_macro`:`train_precision_macro` - Precision-ის შეფასება

* `test_accuracy`:`train_accuracy` - Accuracy-ის შეფასება

### არტეფაქტების აღწერა

მხოლოდ არის Training run-ებში 

* `graph/` - მოდელების გრაფიკების დირექტორია:

  * `top_20_feature_importances.png` - საუკეთესო 20 feature importance-ის გრაფიკი

  * `roc.png` - ROC AUC-ის გრაფიკი

  * `confusion_matrix.png` - Confusion Matrix-ის გრაფიკი

  * `shap_summary_plot.png` - SHAP-ის გრაფიკი

  * `precision_recall_plot.png` - Precision-Recall Curve-ის გრაფიკი

  * `decision_tree.png` - Decision Tree-ის ვიზუალიზაცია

* `model/` - მოდელის არქიტექტურის დირექტორია

* `search/search_results.csv` - RandomSearchCV-ს შედეგების CSV

### საუკეთესო მოდელის შედეგები

#### `model`: xgboost_search_best/2

| Metric                  | Test Value         | Train Value        |
|-------------------------|--------------------|--------------------|
| ROC AUC                 | 0.9123185873376901 | 0.9273033710180494 |
| F1 Macro                | 0.6079844849822877 | 0.6132336683459775 |
| Recall Macro            | 0.8330094675995315 | 0.8472388708225962 |
| Accuracy                | 0.8638354299185075 | 0.8654544407638479 |
| Precision Macro         | 0.5848813142439506 | 0.5883622399275729 |